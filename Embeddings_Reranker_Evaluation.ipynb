{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyushmishra908/50Algorithms/blob/main/Embeddings_Reranker_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42fc5c20-860d-46ac-834e-83c3773682c4",
      "metadata": {
        "id": "42fc5c20-860d-46ac-834e-83c3773682c4"
      },
      "source": [
        "# Boosting RAG: Picking the Best Embedding & Reranker models\n",
        "\n",
        "When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers.\n",
        "\n",
        "But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most?\n",
        "\n",
        "In this tutorial Notebook, we'll use the `Retrieval Evaluation` module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in!\n",
        "\n",
        "Let’s first start with understanding the metrics available in `Retrieval Evaluation`\n",
        "\n",
        "**Understanding Metrics in Retrieval Evaluation**:\n",
        "\n",
        "To gauge the efficacy of our retrieval system, we primarily relied on two widely-accepted metrics: **Hit Rate** and **Mean Reciprocal Rank (MRR)**. Let's delve into these metrics to understand their significance and how they operate.\n",
        "\n",
        "- **Hit Rate**:\n",
        "    \n",
        "    Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it's about how often our system gets it right within the top few guesses.\n",
        "    \n",
        "- **Mean Reciprocal Rank (MRR)**:\n",
        "    \n",
        "    For each query, MRR evaluates the system's accuracy by looking at the rank of the highest-placed relevant document. Specifically, it's the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it's second, the reciprocal rank is 1/2, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c05bfb0a-369c-444a-bd30-2bd263a55c93",
      "metadata": {
        "scrolled": true,
        "id": "c05bfb0a-369c-444a-bd30-2bd263a55c93"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915a3c6b-0e43-4461-a86e-64007ebf9ae5",
      "metadata": {
        "id": "915a3c6b-0e43-4461-a86e-64007ebf9ae5"
      },
      "source": [
        "## Setting Up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6989d8-25cc-4c53-b42b-3c9315a831ca",
      "metadata": {
        "id": "5e6989d8-25cc-4c53-b42b-3c9315a831ca"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "# LLM\n",
        "from llama_index.llms import Anthropic\n",
        "\n",
        "# Embeddings\n",
        "from llama_index.embeddings import OpenAIEmbedding, HuggingFaceEmbedding, CohereEmbedding\n",
        "from langchain.embeddings import VoyageEmbeddings\n",
        "\n",
        "# Retrievers\n",
        "from llama_index.retrievers import (\n",
        "    BaseRetriever,\n",
        "    VectorIndexRetriever,\n",
        ")\n",
        "\n",
        "# Rerankers\n",
        "from llama_index.indices.query.schema import QueryBundle, QueryType\n",
        "from llama_index.schema import NodeWithScore\n",
        "from llama_index.indices.postprocessor.cohere_rerank import CohereRerank\n",
        "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
        "from llama_index.finetuning.embeddings.common import EmbeddingQAFinetuneDataset\n",
        "\n",
        "# Evaluator\n",
        "from llama_index.evaluation import (\n",
        "    generate_question_context_pairs,\n",
        "    EmbeddingQAFinetuneDataset,\n",
        ")\n",
        "from llama_index.evaluation import RetrieverEvaluator\n",
        "\n",
        "\n",
        "from typing import List\n",
        "import pandas as pd\n",
        "import openai\n",
        "import voyageai\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f650d35-fbd6-4e4e-968d-808ff9fbb923",
      "metadata": {
        "id": "3f650d35-fbd6-4e4e-968d-808ff9fbb923"
      },
      "source": [
        "## Settingup API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5313e42-d71c-4427-a0e2-0578f3831c4c",
      "metadata": {
        "id": "f5313e42-d71c-4427-a0e2-0578f3831c4c"
      },
      "outputs": [],
      "source": [
        "openai_api_key = 'YOUR OPENAI API KEY'\n",
        "cohere_api_key = 'YOUR COHEREAI API KEY'\n",
        "anthropic_api_key = 'YOUR ANTHROPIC API KEY'\n",
        "voyage_api_key = 'YOUR VOYAGE API KEY'\n",
        "openai.api_key = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d54c37b-3d4f-4b99-83fd-f98e97f2bcda",
      "metadata": {
        "id": "2d54c37b-3d4f-4b99-83fd-f98e97f2bcda"
      },
      "source": [
        "## Download Data\n",
        "\n",
        "We will use Llama2 paper for this experiment. Let's download the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef4be8e-f307-45a2-8dcf-319738b55051",
      "metadata": {
        "id": "5ef4be8e-f307-45a2-8dcf-319738b55051"
      },
      "outputs": [],
      "source": [
        "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28633c1f-86ba-4272-afc9-ddd3a17c8593",
      "metadata": {
        "id": "28633c1f-86ba-4272-afc9-ddd3a17c8593"
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "Let’s load the data. We will use Pages from start to 36 for the experiment which excludes table of contents, references and appendix.\n",
        "\n",
        "This data was then parsed by converted to nodes, which represent chunks of data we'd like to retrieve. We did use chunk_size as 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f367cc-7ed9-4cf3-9f19-de223eec0841",
      "metadata": {
        "id": "43f367cc-7ed9-4cf3-9f19-de223eec0841"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(input_files=['llama2.pdf']).load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3971e2d-2b91-4bfa-bb77-c87fef94d8af",
      "metadata": {
        "id": "e3971e2d-2b91-4bfa-bb77-c87fef94d8af"
      },
      "outputs": [],
      "source": [
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
        "nodes = node_parser.get_nodes_from_documents(documents[:37])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f83542-cc39-45fa-a728-c8a94f3e1be3",
      "metadata": {
        "id": "22f83542-cc39-45fa-a728-c8a94f3e1be3"
      },
      "source": [
        "## Generating Question-Context Pairs\n",
        "\n",
        "For evaluation purposes, we created a dataset of question-context pairs. This dataset can be seen as a set of questions and their corresponding context from our data. To remove bias for evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31753606-fece-4c64-9152-b7623be16bca",
      "metadata": {
        "id": "31753606-fece-4c64-9152-b7623be16bca"
      },
      "source": [
        "Let's initialize prompt template to generate question-context pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a231e0-06cd-4027-8cb9-f88ea1910b93",
      "metadata": {
        "id": "66a231e0-06cd-4027-8cb9-f88ea1910b93"
      },
      "outputs": [],
      "source": [
        "# Prompt to generate questions\n",
        "qa_generate_prompt_tmpl = \"\"\"\\\n",
        "Context information is below.\n",
        "\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "\n",
        "Given the context information and not prior knowledge.\n",
        "generate only questions based on the below query.\n",
        "\n",
        "You are a Professor. Your task is to setup \\\n",
        "{num_questions_per_chunk} questions for an upcoming \\\n",
        "quiz/examination. The questions should be diverse in nature \\\n",
        "across the document. The questions should not contain options, not start with Q1/ Q2. \\\n",
        "Restrict the questions to the context information provided.\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd8318d-68a2-4808-934a-6302501fae7b",
      "metadata": {
        "id": "ffd8318d-68a2-4808-934a-6302501fae7b"
      },
      "outputs": [],
      "source": [
        "llm = Anthropic(api_key=anthropic_api_key, temperature=0)\n",
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes, llm=llm, num_questions_per_chunk=2, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee59cb1-0d42-4bc5-bd34-b053450463b4",
      "metadata": {
        "id": "1ee59cb1-0d42-4bc5-bd34-b053450463b4"
      },
      "source": [
        "Function to filter out sentences such as `Here are 2 questions based on provided context`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290473b1-c568-490e-9526-fb4032a8de09",
      "metadata": {
        "id": "290473b1-c568-490e-9526-fb4032a8de09"
      },
      "outputs": [],
      "source": [
        "# function to clean the dataset\n",
        "def filter_qa_dataset(qa_dataset):\n",
        "    \"\"\"\n",
        "    Filters out queries from the qa_dataset that contain certain phrases and the corresponding\n",
        "    entries in the relevant_docs, and creates a new EmbeddingQAFinetuneDataset object with\n",
        "    the filtered data.\n",
        "\n",
        "    :param qa_dataset: An object that has 'queries', 'corpus', and 'relevant_docs' attributes.\n",
        "    :return: An EmbeddingQAFinetuneDataset object with the filtered queries, corpus and relevant_docs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract keys from queries and relevant_docs that need to be removed\n",
        "    queries_relevant_docs_keys_to_remove = {\n",
        "        k for k, v in qa_dataset.queries.items()\n",
        "        if 'Here are 2' in v or 'Here are two' in v\n",
        "    }\n",
        "\n",
        "    # Filter queries and relevant_docs using dictionary comprehensions\n",
        "    filtered_queries = {\n",
        "        k: v for k, v in qa_dataset.queries.items()\n",
        "        if k not in queries_relevant_docs_keys_to_remove\n",
        "    }\n",
        "    filtered_relevant_docs = {\n",
        "        k: v for k, v in qa_dataset.relevant_docs.items()\n",
        "        if k not in queries_relevant_docs_keys_to_remove\n",
        "    }\n",
        "\n",
        "    # Create a new instance of EmbeddingQAFinetuneDataset with the filtered data\n",
        "    return EmbeddingQAFinetuneDataset(\n",
        "        queries=filtered_queries,\n",
        "        corpus=qa_dataset.corpus,\n",
        "        relevant_docs=filtered_relevant_docs\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c0f70e6-eb3f-4823-b457-64e08ee7612a",
      "metadata": {
        "id": "9c0f70e6-eb3f-4823-b457-64e08ee7612a"
      },
      "outputs": [],
      "source": [
        "# filter out pairs with phrases `Here are 2 questions based on provided context`\n",
        "qa_dataset = filter_qa_dataset(qa_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f757e90-3f95-4c1d-ba2a-e172a0fd7645",
      "metadata": {
        "id": "4f757e90-3f95-4c1d-ba2a-e172a0fd7645"
      },
      "source": [
        "## Initialize Embeddings and Retrievers\n",
        "\n",
        "Do note that JinaAI embeddings requires compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aab9d1ec-2ca4-4cd3-bd8b-0dde31f51d73",
      "metadata": {
        "id": "aab9d1ec-2ca4-4cd3-bd8b-0dde31f51d73"
      },
      "outputs": [],
      "source": [
        "# Define all embeddings and rerankers\n",
        "EMBEDDINGS = {\n",
        "    \"OpenAI\": OpenAIEmbedding(),\n",
        "    \"bge-large\": HuggingFaceEmbedding(model_name='BAAI/bge-large-en'),\n",
        "    \"llm-embedder\": HuggingFaceEmbedding(model_name='BAAI/llm-embedder'),\n",
        "    \"CohereV2\": CohereEmbedding(cohere_api_key=cohere_api_key, model_name='embed-english-v2.0'),\n",
        "    \"CohereV3\": CohereEmbedding(cohere_api_key=cohere_api_key, model_name='embed-english-v3.0', input_type='search_document'),\n",
        "    \"Voyage\": VoyageEmbeddings(voyage_api_key=voyage_api_key),\n",
        "    # \"JinaAI\": HuggingFaceEmbedding(model_name='jinaai/jina-embeddings-v2-small-en', trust_remote_code=True),\n",
        "}\n",
        "\n",
        "RERANKERS = {\n",
        "    \"WithoutReranker\": \"None\",\n",
        "    \"CohereRerank\": CohereRerank(api_key=cohere_api_key, top_n=5),\n",
        "    \"bge-reranker-base\": SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\", top_n=5),\n",
        "    \"bge-reranker-large\": SentenceTransformerRerank(model=\"BAAI/bge-reranker-large\", top_n=5)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5533b3-eb9c-4628-b197-a23826178a84",
      "metadata": {
        "id": "6a5533b3-eb9c-4628-b197-a23826178a84"
      },
      "source": [
        "### Define a function to display results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bdc5076-7806-4cec-a168-a18f1639a5b8",
      "metadata": {
        "id": "1bdc5076-7806-4cec-a168-a18f1639a5b8"
      },
      "outputs": [],
      "source": [
        "def display_results(embedding_name, reranker_name, eval_results):\n",
        "    \"\"\"Display results from evaluate.\"\"\"\n",
        "\n",
        "    metric_dicts = []\n",
        "    for eval_result in eval_results:\n",
        "        metric_dict = eval_result.metric_vals_dict\n",
        "        metric_dicts.append(metric_dict)\n",
        "\n",
        "    full_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "    hit_rate = full_df[\"hit_rate\"].mean()\n",
        "    mrr = full_df[\"mrr\"].mean()\n",
        "\n",
        "    metric_df = pd.DataFrame(\n",
        "        {\"Embedding\": [embedding_name], \"Reranker\": [reranker_name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n",
        "    )\n",
        "\n",
        "    return metric_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1106e7-1135-461e-82a6-8ac758d60ab0",
      "metadata": {
        "id": "ba1106e7-1135-461e-82a6-8ac758d60ab0"
      },
      "source": [
        "## Define Retriever and Evaluate\n",
        "\n",
        "To identify the optimal retriever, we employ a combination of an embedding model and a reranker. Initially, we establish a base VectorIndexRetriever. Upon retrieving the nodes, we then introduce a reranker to further refine the results. It's worth noting that for this particular experiment, we've set similarity_top_k to 5. However, feel free to adjust this parameter based on the needs of your specific experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb39aa8f-d79f-4d4e-82e7-fa7289c86667",
      "metadata": {
        "id": "bb39aa8f-d79f-4d4e-82e7-fa7289c86667"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame()\n",
        "\n",
        "# Loop over embeddings\n",
        "for embed_name, embed_model in EMBEDDINGS.items():\n",
        "\n",
        "    service_context = ServiceContext.from_defaults(llm=None, embed_model=embed_model)\n",
        "    vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
        "\n",
        "    if embed_name != 'CohereV3':\n",
        "        vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context)\n",
        "    else:\n",
        "        embed_model = CohereEmbedding(cohere_api_key=cohere_api_key, model_name='embed-english-v3.0', input_type='search_query')\n",
        "        service_context = ServiceContext.from_defaults(llm=None, embed_model=embed_model)\n",
        "        vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context)\n",
        "\n",
        "    # Loop over rerankers\n",
        "    for rerank_name, reranker in RERANKERS.items():\n",
        "\n",
        "        print(f\"Running Evaluation for Embedding Model: {embed_name} and Reranker: {rerank_name}\")\n",
        "\n",
        "        # Define Retriever\n",
        "        class CustomRetriever(BaseRetriever):\n",
        "            \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
        "\n",
        "            def __init__(\n",
        "                self,\n",
        "                vector_retriever: VectorIndexRetriever,\n",
        "            ) -> None:\n",
        "                \"\"\"Init params.\"\"\"\n",
        "\n",
        "                self._vector_retriever = vector_retriever\n",
        "\n",
        "            def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "                \"\"\"Retrieve nodes given query.\"\"\"\n",
        "\n",
        "                retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
        "\n",
        "                if reranker != 'None':\n",
        "                    retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
        "                else:\n",
        "                    retrieved_nodes = retrieved_nodes[:5]\n",
        "\n",
        "                return retrieved_nodes\n",
        "\n",
        "            async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "                \"\"\"Asynchronously retrieve nodes given query.\n",
        "\n",
        "                Implemented by the user.\n",
        "\n",
        "                \"\"\"\n",
        "                return self._retrieve(query_bundle)\n",
        "\n",
        "            async def aretrieve(self, str_or_query_bundle: QueryType) -> List[NodeWithScore]:\n",
        "                if isinstance(str_or_query_bundle, str):\n",
        "                    str_or_query_bundle = QueryBundle(str_or_query_bundle)\n",
        "                return await self._aretrieve(str_or_query_bundle)\n",
        "\n",
        "        custom_retriever = CustomRetriever(vector_retriever)\n",
        "\n",
        "        retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "            [\"mrr\", \"hit_rate\"], retriever=custom_retriever\n",
        "        )\n",
        "        eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
        "\n",
        "        current_df = display_results(embed_name, rerank_name, eval_results)\n",
        "        results_df = pd.concat([results_df, current_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b5cde62-5923-4caa-a6c3-c0c081716a73",
      "metadata": {
        "id": "0b5cde62-5923-4caa-a6c3-c0c081716a73"
      },
      "source": [
        "## Check Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605bb11c-97f2-4405-9603-9cacd44fb535",
      "metadata": {
        "id": "605bb11c-97f2-4405-9603-9cacd44fb535"
      },
      "outputs": [],
      "source": [
        "# Display final results\n",
        "print(results_df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}